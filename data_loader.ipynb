{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "img_data = './data/img_data'\n",
    "text_data = './data/text_data'\n",
    "\n",
    "img_features = np.asarray(h5py.File(os.path.join(img_data, 'IR_image_features.h5'), 'r')['img_features'])\n",
    "\n",
    "with open(os.path.join(img_data, 'IR_img_features2id.json'), 'r') as f:\n",
    "     visual_feat_mapping = json.load(f)['IR_imgid2id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_json(os.path.join(text_data, 'Easy/IR_train_easy.json'))\n",
    "# df = df.T.sort_index()\n",
    "# df.dialog = df.dialog.apply(lambda arr: ' '.join([sent for item in arr for sent in item]))\n",
    "# df['all_words'] = df[['caption', 'dialog']].apply(lambda x: x[0] + ' ' + x[1], axis = 1)\n",
    "# df = df[['caption', 'dialog', 'all_words', 'img_list', 'target', 'target_img_id']]\n",
    "# s = df.apply(lambda x: pd.Series(x['img_list']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "# s.name = 'img_id'\n",
    "# df = df.drop('img_list', axis=1).join(s).drop('target', axis=1).reset_index(drop= True)\n",
    "# df['target_onehot'] = np.where(df['target_img_id'] == df['img_id'], 1, 0)\n",
    "# s = []\n",
    "# for img_id in df.img_id.iteritems():\n",
    "#     h5_id = visual_feat_mapping[str(img_id[1])]\n",
    "#     img_feat = img_features[h5_id]\n",
    "#     s.append(img_feat)\n",
    "\n",
    "# df['img_feat'] = pd.Series(s)\n",
    "# df[['all_words', 'img_feat', 'target_onehot']].to_csv('./data/train_data_easy_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df_val = pd.read_json(os.path.join(text_data, 'Easy/IR_val_easy.json'))\n",
    "# df_val = df_val.T.sort_index()\n",
    "# df_val.dialog = df_val.dialog.apply(lambda arr: ' '.join([sent for item in arr for sent in item]))\n",
    "# df_val['all_words'] = df_val[['caption', 'dialog']].apply(lambda x: x[0] + ' ' + x[1], axis = 1)\n",
    "# df_val = df_val[['caption', 'dialog', 'all_words', 'img_list', 'target', 'target_img_id']]\n",
    "# s = df_val.apply(lambda x: pd.Series(x['img_list']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "# s.name = 'img_id'\n",
    "# df_val = df_val.drop('img_list', axis=1).join(s).drop('target', axis=1).reset_index(drop= True)\n",
    "# df_val['target_onehot'] = np.where(df_val['target_img_id'] == df_val['img_id'], 1, 0)\n",
    "# s = []\n",
    "# for img_id in df_val.img_id.iteritems():\n",
    "#     h5_id = visual_feat_mapping[str(img_id[1])]\n",
    "#     img_feat = img_features[h5_id]\n",
    "#     s.append(img_feat)\n",
    "\n",
    "# df_val['img_feat'] = pd.Series(s)\n",
    "# df_val[['all_words', 'img_feat', 'target_onehot']].to_csv('./data/val_data_easy_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_json(os.path.join(text_data, 'Easy/IR_test_easy.json'))\n",
    "# df_test = df_test.T.sort_index()\n",
    "# df_test.dialog = df_test.dialog.apply(lambda arr: ' '.join([sent for item in arr for sent in item]))\n",
    "# df_test['all_words'] = df_test[['caption', 'dialog']].apply(lambda x: x[0] + ' ' + x[1], axis = 1)\n",
    "# df_test = df_test[['caption', 'dialog', 'all_words', 'img_list', 'target', 'target_img_id']]\n",
    "# s = df_test.apply(lambda x: pd.Series(x['img_list']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "# s.name = 'img_id'\n",
    "# df_test = df_test.drop('img_list', axis=1).join(s).drop('target', axis=1).reset_index(drop= True)\n",
    "# df_test['target_onehot'] = np.where(df_test['target_img_id'] == df_test['img_id'], 1, 0)\n",
    "# s = []\n",
    "# for img_id in df_test.img_id.iteritems():\n",
    "#     h5_id = visual_feat_mapping[str(img_id[1])]\n",
    "#     img_feat = img_features[h5_id]\n",
    "#     s.append(img_feat)\n",
    "\n",
    "# df_test['img_feat'] = pd.Series(s)\n",
    "# df_test[['all_words', 'img_feat', 'target_onehot']].to_csv('./data/test_data_easy_bow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train easy\n",
      "train hard\n",
      "val easy\n",
      "val hard\n",
      "test easy\n",
      "test hard\n"
     ]
    }
   ],
   "source": [
    "for data in ['train', 'val', 'test']:\n",
    "    for complexity in ['easy', 'hard']:\n",
    "        print(data, complexity)\n",
    "        df = pd.read_json(os.path.join(text_data, '{0}/IR_{1}_{2}.json'\\\n",
    "                                       .format(complexity.capitalize(), data, complexity)))\n",
    "        df = df.T.sort_index()\n",
    "        df.dialog = df.dialog\\\n",
    "            .apply(lambda arr: ' \\n '.join([sent for item in arr for sent in item]))\n",
    "        df['all_words'] = df[['caption', 'dialog']]\\\n",
    "            .apply(lambda x: x[0] + ' \\n ' + x[1], axis = 1)\n",
    "        df = df[['caption', 'dialog', 'all_words', 'img_list', 'target', 'target_img_id']]\n",
    "        s = df.apply(lambda x: pd.Series(x['img_list']),axis=1)\\\n",
    "            .stack()\\\n",
    "            .reset_index(level=1, drop=True)\n",
    "        s.name = 'img_id'\n",
    "        df = df.drop('img_list', axis=1).join(s).drop('target', axis=1).reset_index(drop= True)\n",
    "        df['target_onehot'] = np.where(df['target_img_id'] == df['img_id'], 1, 0)\n",
    "        \n",
    "        df[['all_words', 'img_id', 'target_onehot']].to_csv('./data/{0}_data_{1}_bow.csv'\\\n",
    "                                                             .format(data, complexity), \n",
    "                                                              index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ImageTextDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        self.imagetext_frame = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagetext_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.imagetext_frame.iloc[idx, 0].split(' \\n ')\n",
    "        img_id = self.imagetext_frame.iloc[idx, 1]\n",
    "        target = self.imagetext_frame.iloc[idx, 2]\n",
    "        h5_id = visual_feat_mapping[str(img_id)]\n",
    "        img_feat = img_features[h5_id]\n",
    "        sample = {'text': text, 'img_features': img_feat, 'target': target}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = ImageTextDataSet(csv_file='./data/train_data_easy_bow.csv')\n",
    "dataloader_train = DataLoader(data_train, batch_size=4,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "data_val = ImageTextDataSet(csv_file='./data/val_data_easy_bow.csv')\n",
    "dataloader_val = DataLoader(data_val, num_workers=4)\n",
    "\n",
    "data_test = ImageTextDataSet(csv_file='./data/test_data_easy_bow.csv')\n",
    "dataloader_test = DataLoader(data_test, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "CBOW\n",
    "\n",
    "Based on Graham Neubig's DyNet code examples:\n",
    "  https://github.com/neubig/nn4nlp2017-code\n",
    "  http://phontron.com/class/nn4nlp2017/\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Functions to read in the corpus\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(dataloader):\n",
    "    for batch in dataloader:\n",
    "        text = batch['text']\n",
    "        for sentences in text:\n",
    "            for sentence in sentences:\n",
    "                resulting_sentence = sentence.lower().strip()\n",
    "                yield ([w2i[x] for x in resulting_sentence.split(\" \")])\n",
    "\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(dataloader_train))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "val = list(read_dataset(dataloader_val))\n",
    "test = list(read_dataset(dataloader_test))\n",
    "\n",
    "nwords = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        bow = torch.sum(embeds, 1)\n",
    "        logits = self.linear(bow)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = CBOW(nwords, 64, ntags)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def evaluate(model, data):\n",
    "    \"\"\"Evaluate a model on a data set.\"\"\"\n",
    "    correct = 0.0\n",
    "    \n",
    "    for words, tag in data:\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        predict = scores.data.numpy().argmax(axis=1)[0]\n",
    "\n",
    "        if predict == tag:\n",
    "            correct += 1\n",
    "\n",
    "    return correct, len(data), correct/len(data)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for ITER in range(100):\n",
    "\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0.0\n",
    "    start = time.time()\n",
    "\n",
    "    for words, tag in train:\n",
    "\n",
    "        # forward pass\n",
    "        lookup_tensor = Variable(torch.LongTensor([words]))\n",
    "        scores = model(lookup_tensor)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        target = Variable(torch.LongTensor([tag]))\n",
    "        output = loss(scores, target)\n",
    "        train_loss += output.data[0]\n",
    "\n",
    "        # backward pass\n",
    "        model.zero_grad()\n",
    "        output.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"iter %r: train loss/sent=%.4f, time=%.2fs\" % \n",
    "          (ITER, train_loss/len(train), time.time()-start))\n",
    "\n",
    "    # evaluate\n",
    "    _, _, acc = evaluate(model, dev)\n",
    "    print(\"iter %r: test acc=%.4f\" % (ITER, acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py-nlp1]",
   "language": "python",
   "name": "conda-env-py-nlp1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
